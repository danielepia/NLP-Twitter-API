{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Relevant News.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNg8vpDmh0ugMh0nsgjH17V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielepia/NLP-Twitter-API/blob/main/Relevant_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbMHyux_02ka",
        "outputId": "7f778434-9fff-45d2-c182-6570f77174d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: advertools in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.7/dist-packages (from advertools) (2.6.1)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from advertools) (0.4.8)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from advertools) (6.0.1)\n",
            "Requirement already satisfied: twython in /usr/local/lib/python3.7/dist-packages (from advertools) (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from advertools) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->advertools) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->advertools) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->advertools) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->advertools) (1.15.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (0.2.1)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (3.3.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.6.2)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.6.0)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (37.0.2)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (4.2.6)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (2.0.5)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.1.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (21.1.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.0.4)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (5.4.0)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (22.0.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (57.4.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (1.22.0)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.7/dist-packages (from scrapy->advertools) (22.4.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy->advertools) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy->advertools) (2.21)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.7/dist-packages (from itemloaders>=1.0.1->scrapy->advertools) (1.0.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy->advertools) (21.4.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy->advertools) (0.2.8)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (21.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (4.2.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (21.0.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (15.1.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy->advertools) (20.2.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy->advertools) (2.10)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy->advertools) (3.7.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy->advertools) (2.23.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy->advertools) (1.5.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy->advertools) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy->advertools) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy->advertools) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython->advertools) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython->advertools) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install advertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EezSRyA31KQe",
        "outputId": "dd7eedbe-7eda-4dff-96c3-2325bbcf0d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.7/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.5.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tweepy\n",
        "import advertools as adv\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import networkx as nx\n",
        "import random"
      ],
      "metadata": {
        "id": "4Z7mJ8f21dHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwitterClient(object):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        auth_params = {\n",
        "            'app_key': 'xxxx',\n",
        "            'app_secret': 'xxxx',\n",
        "            'oauth_token': 'xxxx-xxxx',\n",
        "            'oauth_token_secret': 'xxxx',\n",
        "            }\n",
        "  \n",
        "        adv.twitter.set_auth_params(**auth_params)\n",
        "  \n",
        "    def clean_tweet(self, tweet):\n",
        "\n",
        "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
        "  \n",
        "    def get_tweet_sentiment_comp(self, tweet):\n",
        "        analyser = SentimentIntensityAnalyzer()\n",
        "        analysis = analyser.polarity_scores(tweet)\n",
        "        val = analysis['compound']\n",
        "        return val\n",
        "\n",
        "    def get_tweet_sentiment(self, val):\n",
        "        if val > 0.5:\n",
        "            return 'positive'\n",
        "        elif val < -.5:\n",
        "            return 'negative'\n",
        "        else:\n",
        "            return 'neutral'\n",
        "\n",
        "\n",
        "    def get_tweets(self, user,field, count = 10):\n",
        "        alerts_list = {user:field}\n",
        "    \n",
        "        for keys,values in alerts_list.items():\n",
        "            df = adv.twitter.get_user_timeline(screen_name=keys,tweet_mode=\"extended\")\n",
        "            df = df[df['tweet_full_text'].str.contains(values,regex=True)]\n",
        " \n",
        "        tweets = []\n",
        "\n",
        "        for tweet in df['tweet_full_text']:\n",
        "            parsed_tweet = {}\n",
        "            parsed_tweet['text'] = tweet\n",
        "            val = self.get_tweet_sentiment_comp(tweet)\n",
        "            parsed_tweet['sentiment_comp'] = val\n",
        "            parsed_tweet['sentiment'] = self.get_tweet_sentiment(val)\n",
        "            tweets.append(parsed_tweet)\n",
        "        return tweets"
      ],
      "metadata": {
        "id": "S7qyViIs1LbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFGslLII2UHs",
        "outputId": "ad6d3da6-86bd-471a-9494-d517d099c5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NLP_Process(object):\n",
        "\n",
        "  def vocabulary(self,corpus):\n",
        "    voc = set()\n",
        "    for sentence in corpus:\n",
        "      sentence = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", sentence).split())\n",
        "      for word in sentence.split():\n",
        "        voc.add(word.lower())\n",
        "    return voc\n",
        "\n",
        "  def preprocess(self,corpus):\n",
        "\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "\n",
        "    sent = []\n",
        "    doc = []\n",
        "\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "\n",
        "    for sentence in corpus:\n",
        "      sentence = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", sentence).split())\n",
        "      for word in sentence.split():\n",
        "        if word.lower() not in stop:\n",
        "          sent.append(word.lower())\n",
        "      doc.append(' '.join(sent))\n",
        "      sent = []\n",
        "    return doc\n",
        "\n",
        "\n",
        "  def cosine_similarity(self,vector1, vector2):\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))\n",
        "\n",
        "  def vectorize_bow(self,corpus):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    X_vec = vectorizer.fit_transform(X)\n",
        "    return X_vec.todense()\n",
        "\n",
        "  def vectorize_tfid(self,corpus,norm=\"l2\",analyzer='word', ngram_range=(1,3), max_features = 500):\n",
        "    vectorizer =  TfidfVectorizer(norm=norm,analyzer=analyzer, ngram_range=ngram_range, max_features = max_features)\n",
        "    tf_idf_matrix = vectorizer.fit_transform(corpus)\n",
        "    return tf_idf_matrix.toarray()\n",
        "\n",
        "  def build_similarity_matrix(self,sentences):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = cosine_distance(sentences[idx1], sentences[idx2])\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "RMyJpQ4p1mpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_news(tweets,top_n = 10):\n",
        "  nlp = NLP_Process()\n",
        "  sentences = tweets\n",
        "  sentences_process = nlp.preprocess(sentences)\n",
        "  vocabulary = nlp.vocabulary(sentences_process)\n",
        "  vector_sentences = nlp.vectorize_tfid(sentences_process,max_features=300)\n",
        "  similarity_matrix = nlp.build_similarity_matrix(vector_sentences)\n",
        "\n",
        "  sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
        "  # print(similarity_matrix.shape)\n",
        "\n",
        "  \n",
        "  # scores = nx.pagerank(sentence_similarity_graph,max_iter=5000,tol=1e-02 )\n",
        "  scores = nx.eigenvector_centrality(sentence_similarity_graph)\n",
        "\n",
        "  ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "\n",
        "\n",
        "  summarize_text = []\n",
        "  for i in range(top_n):\n",
        "    print(ranked_sentence[i][1])\n",
        "\n",
        "      \n",
        "def main():\n",
        "\n",
        "    api = TwitterClient()\n",
        "    \n",
        "\n",
        "    \n",
        "    sites = ['reuters','wsj']\n",
        "    tweet_positive = []\n",
        "    tweet_negative = []\n",
        "\n",
        "\n",
        "    for site in sites:\n",
        "      tweets = pd.DataFrame(api.get_tweets(user=site,field=''))\n",
        "      for x,val in zip(tweets['text'].values,tweets['sentiment'].values):\n",
        "        if val == 'positive':\n",
        "          tweet_positive.append(x)\n",
        "        elif val == 'negative':\n",
        "          tweet_negative.append(x)\n",
        "\n",
        "\n",
        "    # tweet = random.shuffle(list(tweet))\n",
        "\n",
        "    print('\\nPositive News')\n",
        "    top_news(tweet_positive)\n",
        "    print('\\n\\nNegative News')\n",
        "    top_news(tweet_negative)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrEaA2Kz1Tzu",
        "outputId": "022a6c24-0efa-4015-b32f-7bb95a6bec9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-26 14:09:55,001 | INFO | twitter.py:238 | wrapper | get_user_timeline | Requesting: count=200, max_id=None, screen_name=reuters, tweet_mode=extended\n",
            "2022-05-26 14:10:04,832 | INFO | twitter.py:238 | wrapper | get_user_timeline | Requesting: count=200, max_id=None, screen_name=wsj, tweet_mode=extended\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Positive News\n",
            "When Champagne Louis Roederer set up shop in California’s Anderson Valley four decades ago, it was a bold move. It’s paid off with delicious wines and a bit of a sparkling boomlet in the valley. https://t.co/LdMO5ezWmP\n",
            "Uvalde, Texas, feels like the only topic worth talking about right now, writes sports columnist @JasonGay https://t.co/BSvZzegqgA\n",
            "UK imposes 25% energy windfall tax to help households as bills surge https://t.co/9gGf7uC4yU https://t.co/2e4cpqsqyo\n",
            "Twitter has agreed to pay $150 million to settle allegations it misused private information, like phone numbers, to target advertising after telling users the information would be used for security reasons https://t.co/CDx0E1v1om https://t.co/iCb0UzL6C0\n",
            "TotalEnergies will buy a 50% stake in Clearway Energy, the latest move by an oil major to expand in wind and solar power https://t.co/zXQMpNh1qr\n",
            "This self-driving truck is demonstrating an impressive precision by autonomously navigating through a maze of fragile China vases https://t.co/VKP3Xqt2Bz\n",
            "The cast and crew of the #Cannes2022 top prize contender ‘Leila's Brothers’ arrive at the red carpet for the world premiere of the Iranian family drama https://t.co/EHlRGuzkhH\n",
            "The Russian economy is being battered by sanctions after its invasion of Ukraine, but the ruble is the best performer against the dollar this year. Here’s why. #WSJWhatsNow https://t.co/kcpG1mwkGt https://t.co/EJ5L5BRkxQ\n",
            "Tens of clowns cheered up Lima’s streets to celebrate the traditional local Clown Day https://t.co/rJahI7IU6N\n",
            "Serco boosts profit outlook as British outsourcer wins more contracts https://t.co/3NkVQ2Gujo https://t.co/0NRT0MTIYR\n",
            "\n",
            "\n",
            "Negative News\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/cluster/util.py:133: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yemen’s economy has been wrecked by years of war and food price inflation doubling in two years. Now, the war in Ukraine and a sudden wheat export ban by India could make a bad situation worse https://t.co/3f3TxtZiha https://t.co/Ny8oh8BI1e\n",
            "Update: The death toll in the Texas school shooting has risen to at least 18 children and one teacher, officials said https://t.co/08Wm7CSX3f\n",
            "Update: The 18-year-old alleged gunman in the Texas school shooting has died, Gov. Greg Abbott said https://t.co/siW4n9C9pl\n",
            "Update: Fourteen children and one teacher were killed in the mass shooting at a Texas elementary school, Gov. Greg Abbott said https://t.co/u22lRMP0nz\n",
            "Ukrainian prosecutors identified eight Russian service members and mercenaries they said were responsible for the killing of a village mayor, her husband and son, who were discovered partly buried in a shallow grave shortly at the end of March https://t.co/ZH4ielw0df\n",
            "UK PM Johnson said he took full responsibility but would not quit after a damning official report detailed a series of illegal lockdown parties at the British leader's Downing Street office https://t.co/hDndnndkbI https://t.co/CfP8NEE8LP\n",
            "Thousands of fish-and-chips shops across Britain are navigating the economic fallout of the Ukraine war, the coronavirus pandemic and Brexit https://t.co/CaDplt1oUX https://t.co/8u8xu2a9fO\n",
            "The worst energy crisis in a half-century is disrupting the West’s transition to cleaner sources of energy, business and government leaders said at this week’s World Economic Forum https://t.co/MY522ff3eS\n",
            "The school shooting in Uvalde, Texas, that left at least 21 dead was the deadliest at a U.S. school since the slaughter at Sandy Hook Elementary. Here's what we know so far. https://t.co/h8C3X5HZ80\n",
            "The decision in Japan to allow in tour groups from countries deemed to be of low Covid-19 risk reflects pressure from business groups that say the economy needs a lift https://t.co/BAvQLdYyYf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z4y7nAAm2dpI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}